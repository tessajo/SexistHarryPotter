{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4090397/321497514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SexistHarryPotter/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevalFunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SexistHarryPotter/evalFunctions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import evalFunctions as ef\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "# kommi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4081827/711812600.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Statistischee Analysen gesamter Text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RESULTS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentenceswithnames.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# with open(os.path.join('Data','RESULTS','test.txt'),'r',encoding='utf-8') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Statistischee Analysen gesamter Text\n",
    "with open(os.path.join('Data','RESULTS','sentenceswithnames.txt'),'r',encoding='utf-8') as f:\n",
    "# with open(os.path.join('Data','RESULTS','test.txt'),'r',encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "# S채tze erstellen\n",
    "sents = nltk.sent_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ef' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4081827/718748952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munigrammtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigramMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigrammtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ef' is not defined"
     ]
    }
   ],
   "source": [
    "unigrammtx = ef.unigramMatrix(sents)\n",
    "\n",
    "print(len(unigrammtx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter auf die Unigramme\n",
    "uallmtx, unamesmtx, uadmtx = ef.ngramFilter(unigrammtx,1,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unimtx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ngramme initialisieren\n",
    "print('unimtx')\n",
    "unimtx_freq,mtx_bigram = ef.initNgrams(unigrammtx)\n",
    "# print('uallmtx')\n",
    "# uall_fmtxreq,uallmtx_bigram = ef.initNgrams(uallmtx)\n",
    "# print('unamesmtx')\n",
    "# unamesmtx_freq,unamesmtx_bigram = ef.initNgrams(unamesmtx)\n",
    "# print('uadmtx')\n",
    "# uadmtx_freq,uadmtx_bigram = ef.initNgrams(uadmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uallmtx_bigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3e6d35a4bf54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;31m# wordnet = nltk.pywordnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m \u001b[0mcreateNetworkGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muallmtx_bigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'uallmtx_bigram' is not defined"
     ]
    }
   ],
   "source": [
    "def createNetworkGraph(bigram):\n",
    "    # Gewichtung\n",
    "    weight = {}\n",
    "    # Tuple-Dictionaries\n",
    "    connection = {}\n",
    "    connection_f = {}\n",
    "    connection_m = {}\n",
    "    # Filter values\n",
    "    fv_names,fv_adv,fv_adj,female,male = ef.getFilterValues(1,0,0,1,1) # funktionuggelt\n",
    "    # Graphen\n",
    "    g = nx.Graph()\n",
    "    g_f = nx.Graph()\n",
    "    g_m = nx.Graph()\n",
    "    # Kennzahlen\n",
    "    weight_of_nodes = []\n",
    "    # Farben Variablen\n",
    "    lblue = '#85B4E4'\n",
    "    dgreen = '#0F8436'\n",
    "    for tuple in bigram:\n",
    "        weight = 0\n",
    "        # TODO Gewichtung 체berpr체fen\n",
    "        # TODO Graphen umstrukturieren\n",
    "        if tuple[0] in fv_names and not tuple[1] in fv_names:\n",
    "            # weiblich\n",
    "            if tuple[0] in female:\n",
    "                connection_f.update({tuple:weight})\n",
    "                weight = connection_f.get(tuple)\n",
    "                weight += 1\n",
    "                connection_f.update({tuple:weight})\n",
    "            # m채nnlich\n",
    "            if tuple[0] in male:\n",
    "                connection_m.update({tuple:weight})\n",
    "                weight = connection_m.get(tuple)\n",
    "                weight += 1\n",
    "                connection_m.update({tuple:weight})\n",
    "            # allgemein\n",
    "            connection.update({tuple:weight})\n",
    "            weight = connection.get(tuple)\n",
    "            weight += 1\n",
    "            connection.update({tuple:weight})\n",
    "        if tuple[1] in fv_names and not tuple[0] in fv_names:\n",
    "            # weiblich\n",
    "            if tuple[0] in female:\n",
    "                n_tuple = (tuple[1],tuple[0])\n",
    "                connection_f.update({n_tuple:weight})\n",
    "                weight = connection_f.get(n_tuple)\n",
    "                weight += 1\n",
    "                connection_f.update({n_tuple:weight})\n",
    "            # m채nnlich\n",
    "            if tuple[0] in male:\n",
    "                n_tuple = (tuple[1],tuple[0])\n",
    "                connection_m.update({n_tuple:weight})\n",
    "                weight = connection_m.get(n_tuple)\n",
    "                weight += 1\n",
    "                connection_m.update({n_tuple:weight})\n",
    "            # allgemein\n",
    "            n_tuple = (tuple[1],tuple[0])\n",
    "            connection.update({n_tuple:weight})\n",
    "            weight = connection.get(n_tuple)\n",
    "            weight += 1\n",
    "            connection.update({n_tuple:weight})\n",
    "    # Alle Personen-Graph\n",
    "    color_map = []\n",
    "    l_of_names = []\n",
    "    l_of_words = []\n",
    "    for key in connection:\n",
    "        value = connection.get(key)\n",
    "        g.add_node(key[0], cluster='noun')\n",
    "        g.add_node(key[1], cluster='ad')\n",
    "        g.add_edge(key[1],key[0],weight=value)\n",
    "        weight_of_nodes.append(value)\n",
    "        # Color-Mapping\n",
    "        if key[1] in fv_names: # Paar aus Namen\n",
    "            # Name 1 bereits erfasst?\n",
    "            if not key[0] in l_of_names: # Wenn nein\n",
    "                color_map.append(dgreen) # TODO Farbe 채ndern\n",
    "                l_of_names.append(key[0])\n",
    "            # Name 2 bereits erfasst?\n",
    "            if not key[1] in l_of_names: # Wenn nein\n",
    "                color_map.append(dgreen)\n",
    "                l_of_names.append(key[1])\n",
    "        else: # nur 1 Name\n",
    "            if not key[0] in l_of_names:\n",
    "                color_map.append(dgreen)\n",
    "                l_of_names.append(key[0])\n",
    "            #Color-Mapping f체r alle anderen W철rter\n",
    "            if not key[1] in l_of_words: \n",
    "                color_map.append(lblue)\n",
    "                l_of_words.append(key[1])\n",
    "    # Assortivit채t-Koeffizient\n",
    "    cr = nx.attribute_assortativity_coefficient(g,\"cluster\",nodes=key)\n",
    "    # Reciprocity\n",
    "    rec = nx.overall_reciprocity(g)\n",
    "    # Transitivity\n",
    "    trans = nx.transitivity(g)\n",
    "    # Clustering\n",
    "    clu = nx.clustering(g)\n",
    "    avgcluster = nx.average_clustering(g)\n",
    "    gdeg = nx.generalized_degree(g)\n",
    "    \n",
    "    print(cr, rec, clu, avgcluster, gdeg)\n",
    "    # Kennzahlen und Algos\n",
    "    # print(weight_of_nodes)\n",
    "    # Graphen darstellen\n",
    "    fig = plt.figure(figsize=(50,20))\n",
    "    subax1 = plt.subplot(122)\n",
    "    nx.draw(g, node_color=color_map, edge_color ='grey', with_labels=True)\n",
    "    subax1.set_title(f\"Wortverkn체pfungen aller Personen\\nAssortivit채tskoeffizient: {cr}\\nReciprocity: {rec}\")\n",
    "    # plt.savefig('wordconnectivity_women.png')\n",
    "    # Frauen-Graph\n",
    "    color_map = []\n",
    "    l_of_names = []\n",
    "    l_of_words = []\n",
    "    weight_of_nodes = []\n",
    "    for key in connection_f:\n",
    "        value = connection_f.get(key)\n",
    "        g_f.add_node(key[0], cluster='noun')\n",
    "        g_f.add_node(key[1], cluster='ad')\n",
    "        g_f.add_edge(key[0],key[1],weight=value)\n",
    "        weight_of_nodes.append(value)\n",
    "        # Color-Mapping\n",
    "        if key[1] in fv_names: # Paar aus Namen\n",
    "            # Name 1 bereits erfasst?\n",
    "            if not key[0] in l_of_names: # Wenn nein\n",
    "                color_map.append(dgreen) # TODO Farbe 채ndern\n",
    "                l_of_names.append(key[0])\n",
    "            # Name 2 bereits erfasst?\n",
    "            if not key[1] in l_of_names: # Wenn nein\n",
    "                color_map.append(dgreen)\n",
    "                l_of_names.append(key[1])\n",
    "        else: # nur 1 Name\n",
    "            if not key[0] in l_of_names:\n",
    "                color_map.append(dgreen)\n",
    "                l_of_names.append(key[0])\n",
    "            #Color-Mapping f체r alle anderen W철rter\n",
    "            if not key[1] in l_of_words: \n",
    "                color_map.append(lblue)\n",
    "                l_of_words.append(key[1])\n",
    "    # assortativity_coefficient\n",
    "    cr_f = nx.attribute_assortativity_coefficient(g_f,\"cluster\",nodes=key)\n",
    "    rec_f = nx.overall_reciprocity(g_f)\n",
    "    print(cr_f, rec_f)\n",
    "    # Kennzahlen & Algos\n",
    "    # print(weight_of_nodes)\n",
    "    fig = plt.figure(figsize= (50,20))\n",
    "    subax1 = plt.subplot(122)\n",
    "    nx.draw(g_f, node_color=color_map, edge_color ='grey', with_labels=True)\n",
    "    subax1.set_title(f\"Wortverkn체pfungen der weiblichen Personen\\nAssortivit채tskoeffizient: {cr_f}\\nReciprocity: {rec_f}\")\n",
    "    # plt.savefig('wordconnectivity_women.png')\n",
    "    # M채nner-Graph\n",
    "    color_map = []\n",
    "    l_of_names = []\n",
    "    l_of_words = []\n",
    "    weight_of_nodes = []\n",
    "    for key in connection_m:\n",
    "        value = connection_m.get(key)\n",
    "        g_m.add_node(key[0], cluster='noun', color='red')\n",
    "        g_m.add_node(key[1], cluster='ad', color='red')\n",
    "        g_m.add_edge(key[0],key[1],weight=value)\n",
    "        weight_of_nodes.append(value)\n",
    "    # Color-Mapping\n",
    "        if key[1] in fv_names: # Paar aus Namen\n",
    "            # Name 1 bereits erfasst?\n",
    "            if not key[0] in l_of_names: # Wenn nein\n",
    "                color_map.append(dgreen)\n",
    "                l_of_names.append(key[0])\n",
    "            # Name 2 bereits erfasst?\n",
    "            if not key[1] in l_of_names: # Wenn nein\n",
    "                color_map.append(dgreen)\n",
    "                l_of_names.append(key[1])\n",
    "        else: # nur 1 Name\n",
    "            if not key[0] in l_of_names:\n",
    "                color_map.append(dgreen)\n",
    "                l_of_names.append(key[0])\n",
    "            #Color-Mapping f체r alle anderen W철rter\n",
    "            if not key[1] in l_of_words: \n",
    "                color_map.append(lblue)\n",
    "                l_of_words.append(key[1])\n",
    "    # assortativity_coefficient\n",
    "    cr_m = nx.attribute_assortativity_coefficient(g_m,\"cluster\",nodes=key)\n",
    "    rec_m = nx.overall_reciprocity(g_m)\n",
    "    print(cr_m, rec_m)\n",
    "    # Kennzahlen & Algos\n",
    "    # print(weight_of_nodes)\n",
    "    fig = plt.figure(figsize= (50,20))\n",
    "    subax1 = plt.subplot(122)\n",
    "    nx.draw(g_m, node_color=color_map, edge_color ='grey', with_labels=True)\n",
    "    subax1.set_title(f\"Wortverkn체pfungen der m채nnlichen Personen\\nAssortivit채tskoeffizient: {cr_m}\\nReciprocity: {rec_m}\")\n",
    "    # plt.savefig('wordconnectivity_men.png')\n",
    "\n",
    "# wordnet = nltk.pywordnet\n",
    "\n",
    "createNetworkGraph(uallmtx_bigram)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24d3916f4a9f8bbe764819496dcbc51ff6063af5fca03b7fe0f93e0003d125b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
